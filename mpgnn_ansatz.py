# -*- coding: utf-8 -*-
"""MPGNN_Ansatz.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1G5-aeTqVt-NjK53lSarU2ahf63Ozj_Nk
"""

import jax
import jax.numpy as jnp
from flax import linen as nn
from typing import Sequence, Tuple

class MlpBlock(nn.Module):
    '''
    Módulo de Flax que define una capa de un MLP.

    Args:
        mlp_dim (int): Dimensión de la capa del MLP.
        dtype (jnp.dtype): Tipo de dato a utilizar.

    Returns:
        jnp.ndarray: Salida de la capa del MLP.

    '''
    mlp_dim: int
    dtype: jnp.dtype = jnp.float32

    @nn.compact
    def __call__(self, x):
        y = nn.Dense(self.mlp_dim, dtype=self.dtype)(x)
        y = nn.swish(y)
        return nn.Dense(x.shape[-1], dtype=self.dtype)(y)

class MLPMessage(nn.Module):
    '''
    Módulo de Flax que define la función entrenable que crea el mensaje, con las
    características de los nodos.

    Args:
        features (Sequence[int]): Lista de dimensiones de las capas del MLP.
        dtype (jnp.dtype): Tipo de dato a utilizar.

    Returns:
        jnp.ndarray: Mensaje construido para cada nodo.
    '''
    features: Sequence[int] = (8, 5, 1)
    dtype: jnp.dtype = jnp.float32
    @nn.compact
    def __call__(self, node_features, edge_features, vecinos, exchange_coefficients):

        origin_input, goal_input = obtain_origin_goal(node_features, edge_features)

        batch = node_features.shape[0]
        edge_features_batch = jnp.broadcast_to(edge_features, (batch,) + edge_features.shape)
        exchange_coefficients_batch = jnp.broadcast_to(exchange_coefficients, (batch,) + exchange_coefficients.shape)

        origin_input = jnp.expand_dims(origin_input, axis=-1)
        goal_input = jnp.expand_dims(goal_input, axis=-1)
        exchange_coefficients_batch = jnp.expand_dims(exchange_coefficients_batch, axis=-1)

        x = jnp.concatenate([origin_input, goal_input, exchange_coefficients_batch], axis=-1)

        def forward_single(x_single, node_features_single):
            for feature in self.features:
                x_single = MlpBlock(feature, dtype=self.dtype)(x_single)
            messages = suma_resultados_con_indices(x_single, node_features_single, vecinos)
            return messages

        batched_forward = jax.vmap(forward_single, in_axes=(0, 0))
        return batched_forward(x, node_features)

class GNNLayer(nn.Module):
    '''
    Módulo de Flax que implementa una capa de paso de mensaje.

    Args:
        N (int): Número de nodos.
        dtype (jnp.dtype): Tipo de dato a utilizar.

    Returns:
        jnp.ndarray: Salida de la capa.
    '''
    N: int
    dtype: jnp.dtype = jnp.float32

    @nn.compact
    def __call__(self, node_features, vecinos, exchange_coefficients):

        #calculamos la matriz de adjacencia y las conexiones (ahora edges)
        adj_matrix = get_adjacency_matrix(self.N)
        edges = get_edge(adj_matrix,self.N)

        #se construyen los mensajes
        messages = MLPMessage(dtype=self.dtype)(node_features, edges, vecinos, exchange_coefficients)

        #combinamos los parametros de spin con mensajes (batch,N)
        combined_features = node_features.at[:, :, 0].add(messages)

        #actualización de nodo mediante función entrenable
        updated_features = MLPMessage(dtype=self.dtype)(combined_features, edges, vecinos, exchange_coefficients)
        updated_features = combined_features.at[:, :, 0].add(updated_features)


        return updated_features

class MPGNN(nn.Module):
    '''
    Módulo de Flax que define una red neuronal de paso de mensaje.

    Args:
        N (int)
        num_message_pass (int): Número de pasos de mensaje.
        vecinos (Tuple[Tuple[int, Tuple[int, ...]]], optional): Array con los vecinos para cada nodo.
        exchange_coefficients (Tuple, optional): Array con los coeficientes de intercambio del hamiltoniano.
        node_components (int, optional): Dimensión de la codificación del nodo.
        dtype (jnp.dtype, optional)

    Returns:
        jnp.ndarray
    '''
    N: int
    num_message_pass: int
    vecinos: Tuple[Tuple[int, Tuple[int, ...]]] = None
    exchange_coefficients: Tuple = None
    node_components: int = 4
    dtype: jnp.dtype = jnp.float32

    @nn.compact
    def __call__(self, node_features):
        if node_features.ndim == 1:
            node_features = node_features[None, ...]

        #definimos e inicializamos los arrays de parametros entrenables
        node_components = self.node_components
        node_array = self.param(
            "node_arrays",
            nn.initializers.normal(),
            (node_features.shape[1], node_components),
            self.dtype
        )
        node_array_batched = jnp.broadcast_to(node_array, (node_features.shape[0], node_features.shape[1], self.node_components))

        #unimos todos los node features
        node_features_expanded = jnp.expand_dims(node_features, axis=-1)
        node_features = jnp.concatenate([node_features_expanded, node_array_batched], axis=-1)

        vecinos = self.vecinos
        exchange_coefficients = self.exchange_coefficients
        exchange_coefficients = jnp.array(exchange_coefficients, dtype=self.dtype)

        #se realizan num_message_pass paso de mensaje
        for _ in range(self.num_message_pass):
            node_features = GNNLayer(self.N, dtype=self.dtype)(node_features, vecinos, exchange_coefficients)
            node_features = nn.swish(node_features)

        #función de agregación
        output = jnp.mean(node_features, axis=(1,2))
        return output

from jax.scipy.special import logsumexp
class sym_MPGNN(nn.Module):
    '''
    Módulo de Flax que incorpora la simetría de inversión a la red.

    Args:
        trivial (bool): Indica si la red es trivial o no.
        N (int)
        num_message_pass (int)
        vecinos (Tuple[Tuple[int, Tuple[int, ...]]])
        exchange_coefficients (Tuple)
        node_components (int)
        dtype (jnp.dtype)

    Returns:
        jnp.ndarray
    '''

    trivial: bool
    N: int
    num_message_pass: int
    vecinos: Tuple[Tuple[int, Tuple[int, ...]]] = None
    exchange_coefficients: Tuple = None
    node_components: int = 4
    dtype: jnp.dtype = jnp.float32

    @nn.compact
    def __call__(self, node_features):
        model = MPGNN(
          N=self.N,
          num_message_pass = self.num_message_pass,
          vecinos=self.vecinos,
          exchange_coefficients =self.exchange_coefficients
      )
        output_x = model(node_features)
        output_inv_x = model(-1*node_features)
        if self.trivial:
            return logsumexp(jnp.array([output_x, output_inv_x]), axis = 0)
        else:
            return logsumexp(jnp.array([output_x, -output_inv_x]), axis = 0)